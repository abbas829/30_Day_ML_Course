{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Explained\n",
    "\n",
    "| **Aspect**                   | **Description**                                                                                             |\n",
    "|------------------------------|-------------------------------------------------------------------------------------------------------------|\n",
    "| **Definition**               | Gradient Boosting is an ensemble learning technique that builds models sequentially, each new model correcting the errors of the previous ones. It's particularly used for regression and classification tasks. |\n",
    "| **How It Works**             | - **Initialization**: Start with an initial prediction, often the mean of the target variable.<br>- **Iterative Process**: In each iteration, fit a new model to the residual errors of the previous model.<br>- **Update Prediction**: Add the new model's predictions to the existing model's predictions to improve accuracy.<br>- **Objective**: Minimize the loss function by adding weak learners. |\n",
    "| **Key Characteristics**      | - **Sequential Learning**: Models are built sequentially, not independently.<br>- **Residuals**: Each new model tries to predict the residuals (errors) of the previous model.<br>- **Learning Rate**: Controls the contribution of each model to the final prediction.<br>- **Additive Model**: Combines the predictions of multiple weak learners (usually decision trees) to form a strong learner. |\n",
    "| **Advantages**               | - **High Accuracy**: Often provides better accuracy than single models.<br>- **Flexibility**: Can be used for regression and classification tasks.<br>- **Robustness to Overfitting**: Can handle overfitting by adjusting hyperparameters such as learning rate and number of trees.<br>- **Feature Importance**: Provides estimates of feature importance, which can be useful for feature selection. |\n",
    "| **Disadvantages**            | - **Computationally Intensive**: Training can be slow, especially with large datasets.<br>- **Complexity**: More complex to implement and tune than simpler models.<br>- **Sensitivity to Hyperparameters**: Requires careful tuning of hyperparameters for optimal performance.<br>- **Prone to Overfitting**: If not properly tuned, it can overfit the training data. |\n",
    "| **Applications**             | - **Finance**: Credit scoring, fraud detection.<br>- **Healthcare**: Disease prediction, patient risk profiling.<br>- **Marketing**: Customer segmentation, churn prediction.<br>- **Energy**: Load forecasting, energy consumption prediction. |\n",
    "| **Types**                    | - **Gradient Boosting for Regression**: Used for predicting continuous values.<br>- **Gradient Boosting for Classification**: Used for predicting categorical outcomes.<br>- **Stochastic Gradient Boosting**: Introduces randomness by subsampling the data to prevent overfitting.<br>- **Regularized Gradient Boosting**: Adds regularization terms to the loss function to improve generalization. |\n",
    "| **Hyperparameters**          | - **n_estimators**: Number of boosting stages (trees) to be added.<br>- **learning_rate**: Shrinks the contribution of each tree to prevent overfitting.<br>- **max_depth**: Maximum depth of the individual trees.<br>- **subsample**: Fraction of samples used for fitting each tree.<br>- **min_samples_split**: Minimum number of samples required to split an internal node.<br>- **min_samples_leaf**: Minimum number of samples required to be at a leaf node. |\n",
    "| **Ensemble Method**          | Combines the predictions of multiple weak learners to create a strong learner. Typically uses decision trees as base learners. |\n",
    "| **Feature Importance**       | Provides estimates of feature importance, which can be useful for understanding the model and feature selection. |\n",
    "| **Implementation Libraries** | - **Python**: scikit-learn (`GradientBoostingClassifier`, `GradientBoostingRegressor`), XGBoost, LightGBM, CatBoost.<br>- **R**: gbm package, xgboost.<br>- **Java**: Weka, H2O.ai.<br>- **Spark**: MLlib Gradient-Boosted Trees. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Booster Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "data = {\n",
    "    'Age': [25, 45, 35, 50, 23, 37, 32, 28, 40, 27],\n",
    "    'Income': [50000, 60000, 70000, 80000, 20000, 30000, 40000, 55000, 65000, 75000],\n",
    "    'Years_Experience': [1, 20, 10, 25, 2, 5, 7, 3, 15, 12],\n",
    "    'Loan_Approved': [0, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables (features) and dependent variable (target)\n",
    "X = df[['Age', 'Income', 'Years_Experience']]\n",
    "y = df['Loan_Approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the gradient boosting model\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "print(f\"Feature Importances:\\n{feature_importances}\")\n",
    "\n",
    "# Plotting the feature importances\n",
    "sns.barplot(x=feature_importances.index, y=feature_importances['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
